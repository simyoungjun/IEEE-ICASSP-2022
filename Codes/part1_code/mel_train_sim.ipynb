{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.io import wavfile\n",
    "from collections import defaultdict, Counter\n",
    "from scipy import signal\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random as rn\n",
    "from keras.layers import Dense\n",
    "from keras import Input\n",
    "#from keras.engine import Model\n",
    "from keras.layers import Dense, TimeDistributed, Dropout, Bidirectional, GRU, BatchNormalization, Activation, LeakyReLU, LSTM, Flatten, RepeatVector, Permute, Multiply, Conv2D, MaxPooling2D\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "# data_dir = '/content/drive/MyDrive/Dataset/audiofile_shorted500/'\n",
    "volume_path = 'C:/Users/GJ/Desktop/연구실/2022SPCUP/WAV_Ordered train set'\n",
    "# volume_path = 'C:/Users/GJ/Desktop/연구실/2022SPCUP/spcup_2022_training_part1'\n",
    "\n",
    "\n",
    "FilePathList =[]\n",
    "\n",
    "for dirName, subdirList, fileList in os.walk(volume_path):\n",
    "    for filename in fileList:\n",
    "        if '.csv' not in filename:\n",
    "            # print(filename)\n",
    "            FilePathList.append(volume_path+'/'+filename)\n",
    "        else:\n",
    "            pd_label = pd.read_csv(volume_path+'/'+filename)\n",
    "\n",
    "labels = np.array(pd_label['algorithm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "##파일마다 1초씩 슬라이싱 해서 data augmentation\n",
    "# X_aug = []\n",
    "\n",
    "sampling_rate = 16000\n",
    "\n",
    "y_labels_aug = []\n",
    "X_mel_aug = []\n",
    "for i, filepath in enumerate(FilePathList):\n",
    "    # fig, ax = plt.subplots()\n",
    "    y, sr = librosa.load(filepath, mono=True, sr = sampling_rate)\n",
    "    index_f = 0\n",
    "    for j in range(y.size//sampling_rate):\n",
    "        X_aug_seg = y[index_f:index_f+sampling_rate]\n",
    "\n",
    "        \n",
    "        S = librosa.feature.melspectrogram(y=X_aug_seg, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        X_mel_aug.append(S_dB)\n",
    "\n",
    "        y_labels_aug.append(labels[i])\n",
    "    \n",
    "        index_f = index_f+sampling_rate\n",
    "        #         X_aug_seg = np.expand_dims(X_aug_seg, axis=0)\n",
    "        #         X_aug.append(X_aug_seg)\n",
    "# X_aug = np.concatenate(X_aug,axis = 0)\n",
    "X_spec_aug = np.array(X_spec_aug)\n",
    "y_labels_aug = np.array(y_labels_aug)\n",
    "print(X_spec_aug.shape)\n",
    "print(y_labels_aug.shape)\n",
    "\n",
    "X = X_spec_aug\n",
    "Y = y_labels_aug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw train_set_num : 4000\n",
      "train set.shape :  (25875, 64, 32)\n",
      "y_train.shape (25875,)\n"
     ]
    }
   ],
   "source": [
    "##파일마다 1초씩 슬라이싱 해서 data augmentation + 마지막에 파일 별로 score 계산하기 위한 preprocess\n",
    "# X_aug = []\n",
    "\n",
    "sampling_rate = 16000\n",
    "n_mels = 64\n",
    "\n",
    "X_train_path, X_test_path, y_train_raw, y_test_raw = train_test_split(np.array(FilePathList), labels, test_size=0.2, stratify = labels, random_state=42)\n",
    "\n",
    "##train set\n",
    "print('raw train_set_num :',len(y_train_raw))\n",
    "\n",
    "y_labels_aug = []\n",
    "X_mel_aug = []\n",
    "for i, filepath in enumerate(X_train_path):\n",
    "    # fig, ax = plt.subplots()\n",
    "    y, sr = librosa.load(filepath, mono=True, sr = sampling_rate)\n",
    "    index_f = 0\n",
    "    for j in range(y.size//sampling_rate):\n",
    "        X_aug_seg = y[index_f:index_f+sampling_rate]\n",
    "\n",
    "        \n",
    "        S = librosa.feature.melspectrogram(y=X_aug_seg, sr=sampling_rate, n_mels=n_mels, fmax=8000)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        X_mel_aug.append(S_dB)\n",
    "\n",
    "        y_labels_aug.append(y_train_raw[i])\n",
    "    \n",
    "        index_f = index_f+sampling_rate\n",
    "        #         X_aug_seg = np.expand_dims(X_aug_seg, axis=0)\n",
    "        #         X_aug.append(X_aug_seg)\n",
    "# X_aug = np.concatenate(X_aug,axis = 0)\n",
    "X_train1 = np.array(X_mel_aug)\n",
    "y_train1 = np.array(y_labels_aug)\n",
    "print('train set.shape : ',X_train1.shape)\n",
    "print('y_train.shape',y_train1.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw test_set_num : 1000\n",
      "splited_test set.shape :  (6423, 64, 32)\n",
      "y_test.shape :  (6423,)\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "##test set\n",
    "print('raw test_set_num :',len(y_test_raw))\n",
    "y_labels_aug = []\n",
    "X_mel_aug = []\n",
    "file_split_num = []\n",
    "for i, filepath in enumerate(X_test_path):\n",
    "    # fig, ax = plt.subplots()\n",
    "    y, sr = librosa.load(filepath, mono=True, sr = sampling_rate)\n",
    "    index_f = 0\n",
    "    for j in range(y.size//sampling_rate):\n",
    "        X_aug_seg = y[index_f:index_f+sampling_rate]\n",
    "\n",
    "        \n",
    "        S = librosa.feature.melspectrogram(y=X_aug_seg, sr=sampling_rate, n_mels=n_mels, fmax=8000)\n",
    "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "        X_mel_aug.append(S_dB)\n",
    "\n",
    "        y_labels_aug.append(y_test_raw[i])\n",
    "    \n",
    "        index_f = index_f+sampling_rate\n",
    "    file_split_num.append(j+1)\n",
    "        \n",
    "X_test1 = np.array(X_mel_aug)\n",
    "y_test1 = np.array(y_labels_aug)\n",
    "print('splited_test set.shape : ',X_test1.shape)\n",
    "print('y_test.shape : ', y_test1.shape)\n",
    "print(len(file_split_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.unique(y_train) [0 1 2 3 4]\n",
      "전체 데이터 label== 0 분포 :  1000\n",
      "전체 데이터 label== 1 분포 :  1000\n",
      "전체 데이터 label== 2 분포 :  1000\n",
      "전체 데이터 label== 3 분포 :  1000\n",
      "전체 데이터 label== 4 분포 :  1000\n",
      "train set label== 0 분포 :  6.252\n",
      "train set label== 1 분포 :  4.779\n",
      "train set label== 2 분포 :  4.634\n",
      "train set label== 3 분포 :  6.127\n",
      "train set label== 4 분포 :  4.083\n",
      "test set label== 0 분포 :  1511\n",
      "test set label== 1 분포 :  1150\n",
      "test set label== 2 분포 :  1219\n",
      "test set label== 3 분포 :  1513\n",
      "test set label== 4 분포 :  1030\n",
      "X_train.shape :  (25875, 64, 32)\n",
      "X_test.shape :  (6423, 64, 32, 1)\n",
      "y_train.shape (25875, 5)\n",
      "y_test.shape :  (6423, 5)\n"
     ]
    }
   ],
   "source": [
    "###Train ,Test data split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(pad_x_arr, label, test_size=0.2, stratify = label, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify = Y, random_state=42)\n",
    "\n",
    "print('np.unique(y_train)',np.unique(y_train1))\n",
    "\n",
    "for i in range(5):\n",
    "    print('전체 데이터 label'+'==',i, '분포 : ', labels.tolist().count(i))\n",
    "for i in range(5):\n",
    "    print('train set label'+'==',i, '분포 : ', y_train1.tolist().count(i)/labels.tolist().count(i))\n",
    "for i in range(5):\n",
    "    print('test set label'+'==',i, '분포 : ', y_test1.tolist().count(i))\n",
    "    \n",
    "X_train_ = X_train1.reshape(X_train1.shape[0],X_train1.shape[1],X_train1.shape[2],1)\n",
    "X_test = X_test1.reshape(X_test1.shape[0],X_test1.shape[1],X_test1.shape[2],1)\n",
    "print('X_train.shape : ',X_train.shape)\n",
    "print('X_test.shape : ',X_test.shape)\n",
    "\n",
    "y_train =to_categorical(y_train1) \n",
    "y_test = to_categorical(y_test1) \n",
    "# print(y_test.shape)\n",
    "print('y_train.shape',y_train.shape)\n",
    "print('y_test.shape : ',y_test.shape)\n",
    "\n",
    "# print('y : ',y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "324/324 [==============================] - 15s 44ms/step - loss: 1.9694 - accuracy: 0.5256 - val_loss: 0.8235 - val_accuracy: 0.7161\n",
      "Epoch 2/50\n",
      "324/324 [==============================] - 14s 43ms/step - loss: 0.6232 - accuracy: 0.7871 - val_loss: 0.5424 - val_accuracy: 0.8139\n",
      "Epoch 3/50\n",
      " 29/324 [=>............................] - ETA: 11s - loss: 0.5191 - accuracy: 0.8125"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-6b09f3960274>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'best_model.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#최상 가중치 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m classifier = model.fit(X_train,\n\u001b[0m\u001b[0;32m     28\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## model compile & fit\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(X_train_.shape[1:])))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 1, mode = 'auto',restore_best_weights=True)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True) #최상 가중치 저장\n",
    "\n",
    "classifier = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=64, callbacks = [early_stopping],\n",
    "                    validation_split = 0.2)\n",
    "\n",
    "plt.plot(classifier.history['accuracy'])\n",
    "plt.plot(classifier.history['val_accuracy'])\n",
    "# plt.plot(classifier.history['loss'])\n",
    "# plt.plot(classifier.history['val_loss'])\n",
    "plt.legend(['val_accuracy', 'accuracy'], loc = 'upper left')\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print('테스트 정확도:', test_acc)\n",
    "\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 0, 2, 2, 0, 0, 1, 0, 2, 2, 1, 0, 0, 2, 4, 0, 2, 1, 3, 4, 2, 4, 4, 0, 3, 1, 2, 0, 3, 4, 3, 1, 1, 0, 3, 0, 2, 2, 4, 0, 0, 3, 2, 0, 2, 3, 4, 1, 3, 1, 1, 3, 1, 2, 3, 3, 0, 3, 3, 1, 0, 2, 0, 1, 3, 4, 3, 3, 1, 4, 1, 0, 2, 0, 0, 4, 1, 4, 0, 0, 2, 2, 4, 1, 0, 3, 2, 1, 4, 3, 2, 2, 3, 2, 3, 2, 4, 1, 3]\n",
      "[1 3 0 2 2 0 0 1 0 2 2 1 0 0 2 4 0 2 1 3 4 2 4 4 0 3 2 2 0 3 4 3 2 2 0 3 0\n",
      " 2 2 4 0 0 3 2 0 2 3 4 1 3 2 1 3 1 2 3 3 0 3 3 1 0 1 0 1 3 4 3 3 2 4 1 0 2\n",
      " 0 0 4 1 4 0 0 2 2 4 1 0 3 1 1 4 3 2 1 3 2 3 2 4 1 3]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True False  True  True  True\n",
      "  True  True  True  True]\n",
      "0.959\n",
      "test_set.size :  1000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "predicted_classes = np.argmax(prediction, axis = 1)\n",
    "f = 0\n",
    "test_predict=[]\n",
    "for i in file_split_num:\n",
    "#     print(predicted_classes[f:f+i])\n",
    "    test_predict.append(mode(predicted_classes[f:f+i])[0][0])\n",
    "    f+=i\n",
    "                    \n",
    "acc_bool = test_predict==y_test_raw\n",
    "print(test_predict[:100])\n",
    "print(y_test_raw[:100])\n",
    "print(acc_bool[:100])\n",
    "test_accuracy = acc_bool.tolist().count(True)/len(acc_bool)\n",
    "print(test_accuracy)\n",
    "print('test_set.size : ', acc_bool.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
